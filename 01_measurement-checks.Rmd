---
title: "Measurement checks — HPT (Czech data)"
subtitle: "Reliability, dimensionality, presentism-contextualization contrast, and ICCs"
author: "HPT and Extremism project"
date: "`r format(Sys.Date())`"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: true
    toc: true
    toc_depth: 3
fontsize: 10pt
geometry:
  - landscape
  - margin=0.7in

header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{threeparttable}
  - \usepackage{threeparttablex}
  - \usepackage[normalem]{ulem}
  - \usepackage{makecell}
  - \setlength{\LTcapwidth}{\textwidth}
---

# What this document does

This report checks whether our **Historical Perspective-Taking (HPT)** instrument behaves well **before** we run any hypothesis tests.

We do four things:

1. **Reliability:** Are the HPT subscales internally consistent? We report **Cronbach’s alpha ($\alpha$)** and **McDonald’s omega ($\omega$)** for the three HPT modes: **POP**, **ROA**, **CONT** (three items each; response scale 1-4).
2. **Dimensionality (CFA/EFA):** Does the **factor structure** match prior research (roughly, **POP+CONT together** versus **ROA** as a separate factor; or three distinct but correlated factors)?
3. **Presentism-contextualization contrast:** Do **POP** (presentist) and **CONT** (contextualization) show the expected contrast in the Czech data (differences in means / correlations)?
4. **Class-level clustering (ICCs):** Are scores clustered by **class** (so that multilevel models are justified later)?

> **Input:** We assume a file `normalised_data.RData` providing an object `normalised_data` with variables `POP1-3`, `ROA1-3`, `CONT1-3`, and `class_label`.
> **Output:** A human-readable PDF with tables/figures and short interpretations.

# Setup and data loading

We load common R packages, then load the preprocessed dataset your pipeline already created.

```{r,setup,message=FALSE, warning=FALSE}
options(width = 120)

# Data handling & plots
library(tidyverse)

# Psychometrics
library(psych)        # alpha, omega, polychoric, EFA helpers
library(lavaan)       # CFA
library(semTools)     # model comparisons & extras

# Multilevel ICCs
library(lme4)
library(performance)

# Tables
library(knitr)


# Make kableExtra use longtable/booktabs and avoid loading tabu
options(kableExtra.latex.load_packages = FALSE)
library(kableExtra)

# Load the dataset created in 00_data-preparation
load("normalised_responses.RData")
stopifnot(exists("normalised_responses"))
dat <- normalised_responses


print_tbl <- function(df, caption, digits = 3, escape = TRUE) {
  kbl(df, booktabs = TRUE, longtable = TRUE, caption = caption, digits = digits, escape = escape) |>
    kable_styling(full_width = FALSE, latex_options = c("hold_position"))
}
```

We verify that **HPT items** and **class labels** exist. If something is missing, we stop with a clear message.

```{r,check-columns}
## -- check-columns ----------------------------
hpt_cols <- c(paste0("POP", 1:3), paste0("ROA", 1:3), paste0("CONT", 1:3))
need   <- c(hpt_cols, "class_label")
miss   <- setdiff(need, names(dat))
if (length(miss)) stop("Missing variables: ", paste(miss, collapse = ", "))

# Keep rows complete on HPT items for psychometric checks
hpt_items <- dat %>% select(all_of(hpt_cols)) %>% drop_na()
n_complete <- nrow(hpt_items)
cat("Rows with complete HPT data:", n_complete, "\n")
```

We create an **analysis dataframe** keeping only rows with complete HPT data and a non-missing `class_label`.
```{r,align-analysis-frame}
# Keep only rows that are COMPLETE on all HPT items AND have a class_label
keep <- complete.cases(dat[, hpt_cols]) & !is.na(dat$class_label)

analysis_df <- dat[keep, c(hpt_cols, "class_label")] %>%
  as_tibble()

nrow_all   <- nrow(dat)
nrow_keep  <- nrow(analysis_df)
cat("Rows in full data: ", nrow_all,  "\n",
    "Rows kept (complete HPT + class_label): ", nrow_keep, "\n", sep = "")
```

# Step 1 — Descriptives and scale construction

**Why:** Simple summaries catch obvious data problems and help readers develop intuition.

* We compute subscale **means** for POP, ROA, CONT (each ranges 1-4).
* We also compute a grand **HPT_total** (mean of the three subscales).
* Then we print summaries and a quick correlation overview.

```{r,descriptives}
hpt_items <- analysis_df %>% select(all_of(hpt_cols))  # 9 HPT items

hpt_scores <- hpt_items %>%
  mutate(
    POP  = rowMeans(select(., starts_with("POP")),  na.rm = TRUE),
    ROA  = rowMeans(select(., starts_with("ROA")),  na.rm = TRUE),
    CONT = rowMeans(select(., starts_with("CONT")), na.rm = TRUE),
    HPT_total = rowMeans(across(c(POP, ROA, CONT)), na.rm = TRUE)
  )

summary(select(hpt_scores, POP, ROA, CONT, HPT_total))
cor(select(hpt_scores, POP, ROA, CONT), use = "pairwise.complete.obs")
```

# Step 2 — Reliability: $\alpha$ and $\omega$ for POP-ROA-CONT

**Why:** Reliability indicates whether items that are supposed to measure the same thing **hang together**.
We report:

* **$\alpha$ (alpha)** on raw item data (common baseline), and
* **$\alpha$ and $\omega$** from **polychoric** correlations (better for ordinal 1-4 items).

Interpretation tip for readers: **$\omega_{\text{total}} \gtrsim .70$** is often seen as acceptable; **$\omega_{\text{hier}}$** indicates strength of a general factor (useful if items might reflect a dominant common trait).

```{r,reliability,warning=FALSE}
alpha_poly <- function(x) {
  pc <- psych::polychoric(x)$rho
  psych::alpha(pc, n.obs = nrow(x))
}
omega_poly <- function(x) {
  pc <- psych::polychoric(x)$rho
  psych::omega(pc, n.obs = nrow(x), nfactors = 1, plot = FALSE)
}

subsets <- list(
  POP  = hpt_items %>% select(starts_with("POP")),
  ROA  = hpt_items %>% select(starts_with("ROA")),
  CONT = hpt_items %>% select(starts_with("CONT"))
)

rel_table <- purrr::imap_dfr(subsets, function(df, nm){
  a_raw  <- psych::alpha(df)
  a_poly <- alpha_poly(df)
  om     <- omega_poly(df)
  tibble(
    scale = nm,
    k_items = ncol(df),
    alpha_raw  = unname(a_raw$total$raw_alpha),
    alpha_poly = unname(a_poly$total$raw_alpha),
    omega_total = unname(om$omega.tot),
    omega_hier  = unname(om$omega.h)
  )
})

print_tbl(rel_table, digits = 3, caption = "Reliability of HPT subscales (alpha and omega).")
```

**How to read this table:** Higher values mean items within a subscale are consistent. If a subscale shows **low $\alpha$ and $\omega$**, consider revisiting items or treating the subscale cautiously in later analyses.

# Step 3 — Dimensionality (CFA/EFA)

**Goal:** Check whether our data reproduce the **structure** reported in prior HPT work (often: **POP+CONT** vs **ROA**, or a **three-factor** model with POP, CONT, ROA as correlated factors).

We fit three **confirmatory factor models** using **ordered** items (WLSMV):

* **M1 (two factors):**
  *F1* loads on `POP1-3` and `CONT1-3`; *F2* loads on `ROA1-3`.
* **M2 (three factors):**
  *POP*, *CONT*, *ROA* as separate but correlated.
* **M3 (one factor):**
  Everything loads on a single general factor.

We then compare model fit and inspect loadings.

```{r,cfa-setup,warning=FALSE}
hpt_ord <- hpt_items  # same data; we explicitly treat items as ordered

m1_2factor <- '
F1 =~ POP1 + POP2 + POP3 + CONT1 + CONT2 + CONT3
F2 =~ ROA1 + ROA2 + ROA3
F1 ~~ F2
'
m2_3factor <- '
POP  =~ POP1 + POP2 + POP3
CONT =~ CONT1 + CONT2 + CONT3
ROA  =~ ROA1 + ROA2 + ROA3
POP ~~ CONT + ROA
CONT ~~ ROA
'
m3_1factor <- '
G =~ POP1 + POP2 + POP3 + ROA1 + ROA2 + ROA3 + CONT1 + CONT2 + CONT3
'

fit_2 <- cfa(m1_2factor, data = hpt_ord, ordered = hpt_cols, estimator = "WLSMV")
fit_3 <- cfa(m2_3factor, data = hpt_ord, ordered = hpt_cols, estimator = "WLSMV")
fit_1 <- cfa(m3_1factor, data = hpt_ord, ordered = hpt_cols, estimator = "WLSMV")

# Compare fits side-by-side
semTools::compareFit(fit_2, fit_3, fit_1)
```

Now we print key indices and standardized loadings for each model.

```{r,cfa-details}
report_fit <- function(fit) {
  list(
    indices = fitMeasures(fit, c("cfi","tli","rmsea","rmsea.ci.lower","rmsea.ci.upper","srmr")),
    loadings = standardizedSolution(fit) %>% as_tibble() %>% filter(op == "=~")
  )
}

cfa_summary <- list(
  `2-factor (POP+CONT vs ROA)` = report_fit(fit_2),
  `3-factor (POP/CONT/ROA)`    = report_fit(fit_3),
  `1-factor (general)`          = report_fit(fit_1)
)

# Print nicely
purrr::iwalk(cfa_summary, function(x, nm){
  cat("\n###", nm, "\n")
  print(x$indices)
  print(kable(x$loadings, digits = 3))
})
```

**How to interpret:** Prefer models with **CFI/TLI $\gtrsim .95$**, **RMSEA $\lesssim .06$-$.08$**, **SRMR $\lesssim .08$** (rules of thumb). If the 2- or 3-factor model clearly outperforms 1-factor and loadings align with expectations (POP & CONT together; ROA separate—or all three distinct), the data support the theorized structure.

### Optional: Data-driven EFA (polychoric)

**Why:** As a sensitivity check, we can inspect **exploratory** factor analysis using polychoric correlations.

```{r,efa}
pc <- psych::polychoric(hpt_ord)$rho
efa2 <- psych::fa(pc, nfactors = 2, fm = "pa", rotate = "oblimin")
efa3 <- psych::fa(pc, nfactors = 3, fm = "pa", rotate = "oblimin")

cat("\nEFA (2 factors):\n")
print(efa2$loadings, cutoff = 0.25)

cat("\nEFA (3 factors):\n")
print(efa3$loadings, cutoff = 0.25)
```

# Step 4 — Presentism-contextualization contrast (POP vs CONT)

**Idea:** Prior work suggests **presentist** choices (POP) and **contextualized** reasoning (CONT) should **pull in opposite directions**. Here we check whether the **Czech data** replicate that **contrast**: (a) compare means; (b) inspect the POP-CONT correlation.

```{r,pop-cont-contrast}
# Rebuild subscale scores locally to avoid scope/version issues
hpt_scores_local <- hpt_items %>%
  mutate(
    POP  = rowMeans(select(., starts_with("POP")),  na.rm = TRUE),
    ROA  = rowMeans(select(., starts_with("ROA")),  na.rm = TRUE),
    CONT = rowMeans(select(., starts_with("CONT")), na.rm = TRUE),
    HPT_total = rowMeans(across(c(POP, ROA, CONT)), na.rm = TRUE)
  )

# Sanity check: make sure the columns exist
stopifnot(all(c("POP","ROA","CONT","HPT_total") %in% names(hpt_scores_local)))

contrast_tbl <- hpt_scores_local %>%
  summarise(
    mean_POP   = mean(POP,  na.rm = TRUE),  sd_POP   = sd(POP,  na.rm = TRUE),
    mean_CONT  = mean(CONT, na.rm = TRUE),  sd_CONT  = sd(CONT, na.rm = TRUE),
    r_POP_CONT = cor(POP, CONT, use = "pairwise.complete.obs")
  )

print_tbl(contrast_tbl, digits = 3, caption = "POP vs CONT: means, SDs, and correlation.")

# Simple paired comparison (descriptive; not a preregistered test)
t_test <- t.test(hpt_scores_local$POP, hpt_scores_local$CONT, paired = TRUE)
t_test

```

**Reading the results:**

* If **mean_CONT $>$ mean_POP** and/or **$r_{\text{POP,CONT}} < 0$**, that supports the expected contrast.
* If they move **together** (positive correlation and similar means), interpretation of the HPT construct may require caution.

# Step 5 — Distribution checks

**Why:** Skewed or piled-up scores can cause model or inference issues. We look at item-level and scale-level histograms.

```{r,distributions}
long_items <- hpt_items %>%
  pivot_longer(cols = everything(), names_to = "item", values_to = "score")

# Item distributions
ggplot(long_items, aes(score)) +
  geom_histogram(binwidth = 0.5, boundary = 0, closed = "left") +
  facet_wrap(~ item, ncol = 3) +
  labs(title = "HPT item score distributions", x = "Score (1-4)", y = "Count")

# Scale distributions
ggplot(hpt_scores %>% pivot_longer(c(POP, ROA, CONT, HPT_total), names_to = "scale", values_to = "score"),
       aes(x = score)) +
  geom_histogram(binwidth = 0.25) +
  facet_wrap(~ scale, scales = "free") +
  labs(title = "Subscale/total score distributions", x = "Mean score (1-4)", y = "Count")
```

# Step 6 — Class-level ICCs (is a multilevel model warranted?)

**Why:** Students are nested in **classes**; scores may be more similar within a class. The **Intraclass Correlation Coefficient (ICC)** estimates the fraction of variance at the class level. If ICC $\gtrsim .05$, multilevel modeling is usually advisable.

We fit **null (random-intercept)** models for **HPT_total**, **POP**, **ROA**, **CONT** and extract ICCs.

```{r,icc}
# analysis_df and hpt_scores already exist and are aligned
icc_data <- analysis_df %>%
  transmute(class_label) %>%
  bind_cols(hpt_scores %>% select(POP, ROA, CONT, HPT_total))

mk_icc <- function(dv){
  f <- reformulate("1 + (1|class_label)", response = dv)
  fit <- lmer(f, data = icc_data, REML = TRUE)

  # Extract variance components
  vc <- as.data.frame(VarCorr(fit))
  var_class <- vc$vcov[vc$grp == "class_label"][1]
  var_resid <- vc$vcov[vc$grp == "Residual"][1]

  icc <- var_class / (var_class + var_resid)

  tibble(
    DV = dv,
    ICC = icc,
    var_class = var_class,
    var_resid = var_resid,
    singular = isSingular(fit)
  )
}

icc_tbl <- purrr::map_dfr(c("HPT_total","POP","ROA","CONT"), mk_icc)

print_tbl(icc_tbl, digits = 3, caption = "Null-model ICCs by outcome (computed from variance components).")

```

**Interpretation:**

* **Higher ICC** $\Rightarrow$ more clustering by class.
* Non-trivial ICCs motivate **multilevel** models for confirmatory analyses.

# Step 7 — Knowledge mini-test (KN1–KN6)

Why: The KN items are dichotomous (0/1). We report:

- KR-20 (equivalent to alpha for dichotomous items)
- Item difficulty (p = proportion correct)
- Point-biserial discrimination (w.r.t. total score)

```{r,kn-items,warning=FALSE}
kn_cols <- paste0("KN", 1:6)
has_kn  <- all(kn_cols %in% names(dat))

if (!has_kn) {
  cat("\n**Knowledge section skipped:** KN1–KN6 not found in data.\n")
} else {
  kn_items <- dat[keep, kn_cols]  # align to analysis_df rows via 'keep'
  # Basic sanity: coerce to numeric 0/1
  kn_items <- kn_items %>% mutate(across(everything(), ~ as.numeric(.)))

  # Total score, difficulty (p), discrimination (point-biserial)
  kn_total <- rowSums(kn_items, na.rm = TRUE)

  item_stats <- tibble(
    item = kn_cols,
    difficulty_p = sapply(kn_items, function(x) mean(x, na.rm = TRUE)),
    discr_pb = sapply(kn_items, function(x) cor(x, kn_total - x, use = "pairwise.complete.obs"))
  )

  # KR-20 (alpha on dichotomous items)
  kn_alpha <- psych::alpha(kn_items)

  print_tbl(item_stats, digits = 3, caption = "KN items: difficulty (p) and point-biserial discrimination.")

  print_tbl(tibble(
    k_items = ncol(kn_items),
    total_mean = mean(kn_total, na.rm = TRUE),
    total_sd   = sd(kn_total, na.rm = TRUE),
    alpha_KR20 = unname(kn_alpha$total$raw_alpha)
  ), digits = 3, caption = "KN total: summary and KR-20 (alpha for dichotomous items).")
}
```

# Step 8 — Ideology batteries (FR-LF mini, KSA-3) and Social Desirability (SDR-5)
Why:

- We need reliable predictors and controls before hypothesis tests.
- We report $\alpha$/$\omega$ (polychoric), optional CFA fits, and descriptive summaries.

```{r,ideology-batteries-helpers,warning=FALSE}
# Helper: reliability table for Likert batteries (polychoric + omega total)
alpha_poly_likert <- function(x) {
  pc <- psych::polychoric(x)$rho
  psych::alpha(pc, n.obs = nrow(x))
}
omega_total_poly_likert <- function(x) {
  pc <- psych::polychoric(x)$rho
  if (!all(eigen(pc, symmetric = TRUE)$values > 1e-6)) pc <- psych::cor.smooth(pc)
  suppressWarnings(psych::omega(pc, n.obs = nrow(x), nfactors = 1, plot = FALSE)$omega.tot)
}
```

## FR-LF mini (RD1-RD3, NS1-NS3) 

```{r,fr-lf-mini,warning=FALSE}
fr_cols <- c(paste0("RD", 1:3), paste0("NS", 1:3))
has_fr  <- all(fr_cols %in% names(dat))

if (!has_fr) {
  cat("\n**FR-LF mini section skipped:** RD1–3 and/or NS1–3 not found.\n")
} else {
  fr_df <- dat[keep, fr_cols] %>% as_tibble()
  RD <- fr_df %>% select(starts_with("RD"))
  NS <- fr_df %>% select(starts_with("NS"))

  fr_rel <- bind_rows(
    {
      a <- psych::alpha(RD); ap <- alpha_poly_likert(RD); wt <- omega_total_poly_likert(RD)
      tibble(scale = "FR-LF: RD", k_items = ncol(RD),
             alpha_raw = a$total$raw_alpha, alpha_poly = ap$total$raw_alpha, omega_total = wt)
    },
    {
      a <- psych::alpha(NS); ap <- alpha_poly_likert(NS); wt <- omega_total_poly_likert(NS)
      tibble(scale = "FR-LF: NS", k_items = ncol(NS),
             alpha_raw = a$total$raw_alpha, alpha_poly = ap$total$raw_alpha, omega_total = wt)
    },
    {
      a <- psych::alpha(fr_df); ap <- alpha_poly_likert(fr_df); wt <- omega_total_poly_likert(fr_df)
      tibble(scale = "FR-LF: total (RD+NS)", k_items = ncol(fr_df),
             alpha_raw = a$total$raw_alpha, alpha_poly = ap$total$raw_alpha, omega_total = wt)
    }
  )

  print_tbl(fr_rel, digits = 3, caption = "FR-LF mini reliability (alpha, polychoric alpha, omega total).")

  # Optional CFA: 2 correlated factors (RD, NS), ordered WLSMV
  fr_model <- '
  RD =~ RD1 + RD2 + RD3
  NS =~ NS1 + NS2 + NS3
  RD ~~ NS
  '
  fr_fit <- try(lavaan::cfa(fr_model, data = fr_df, ordered = colnames(fr_df), estimator = "WLSMV"), silent = TRUE)
  if (!inherits(fr_fit, "try-error")) {
    print(fitMeasures(fr_fit, c("cfi","tli","rmsea","srmr")))
  } else {
    cat("\nFR-LF CFA skipped (model failed to converge).\n")
  }
}
```

## KSA-3 (A1-A3, U1-U3, K1-K3)

```{r,ksa-3,warning=FALSE}
ksa_cols <- c(paste0("A",1:3), paste0("U",1:3), paste0("K",1:3))
has_ksa  <- all(ksa_cols %in% names(dat))

if (!has_ksa) {
  cat("\n**KSA-3 section skipped:** A1–A3, U1–U3, and/or K1–K3 not found.\n")
} else {
  ksa_df <- dat[keep, ksa_cols] %>% as_tibble()
  A <- ksa_df %>% select(starts_with("A"))
  U <- ksa_df %>% select(starts_with("U"))
  K <- ksa_df %>% select(starts_with("K"))

  ksa_rel <- bind_rows(
    {
      a <- psych::alpha(A); ap <- alpha_poly_likert(A); wt <- omega_total_poly_likert(A)
      tibble(scale = "KSA-3: Aggression (A)", k_items = 3,
             alpha_raw = a$total$raw_alpha, alpha_poly = ap$total$raw_alpha, omega_total = wt)
    },
    {
      a <- psych::alpha(U); ap <- alpha_poly_likert(U); wt <- omega_total_poly_likert(U)
      tibble(scale = "KSA-3: Submission (U)", k_items = 3,
             alpha_raw = a$total$raw_alpha, alpha_poly = ap$total$raw_alpha, omega_total = wt)
    },
    {
      a <- psych::alpha(K); ap <- alpha_poly_likert(K); wt <- omega_total_poly_likert(K)
      tibble(scale = "KSA-3: Conventionalism (K)", k_items = 3,
             alpha_raw = a$total$raw_alpha, alpha_poly = ap$total$raw_alpha, omega_total = wt)
    },
    {
      a <- psych::alpha(ksa_df); ap <- alpha_poly_likert(ksa_df); wt <- omega_total_poly_likert(ksa_df)
      tibble(scale = "KSA-3: total", k_items = 9,
             alpha_raw = a$total$raw_alpha, alpha_poly = ap$total$raw_alpha, omega_total = wt)
    }
  )

  print_tbl(ksa_rel, digits = 3, caption = "KSA-3 reliability (alpha, polychoric alpha, omega total).")

  # Optional CFA: 3 correlated factors (A, U, K)
  ksa_model <- '
  A =~ A1 + A2 + A3
  U =~ U1 + U2 + U3
  K =~ K1 + K2 + K3
  A ~~ U + K
  U ~~ K
  '
  ksa_fit <- try(lavaan::cfa(ksa_model, data = ksa_df, ordered = colnames(ksa_df), estimator = "WLSMV"), silent = TRUE)
  if (!inherits(ksa_fit, "try-error")) {
    print(fitMeasures(ksa_fit, c("cfi","tli","rmsea","srmr")))
  } else {
    cat("\nKSA-3 CFA skipped (model failed to converge).\n")
  }
}
```

## SDR-5 (SDR1-SDR5)

```{r,sdr-5,warning=FALSE}
sdr_cols <- paste0("SDR", 1:5)
has_sdr  <- all(sdr_cols %in% names(dat))

if (!has_sdr) {
  cat("\n**SDR-5 section skipped:** SDR1–SDR5 not found.\n")
} else {
  sdr_df <- dat[keep, sdr_cols] %>% as_tibble()
  # NOTE: Your data reportedly already has SDR2–SDR4 reversed. If unsure, you can
  # check symmetry and optionally reverse here before reliability.
  a_sdr  <- psych::alpha(sdr_df)
  ap_sdr <- alpha_poly_likert(sdr_df)
  wt_sdr <- omega_total_poly_likert(sdr_df)

  print_tbl(tibble(
    scale = "SDR-5",
    k_items = 5,
    alpha_raw = a_sdr$total$raw_alpha,
    alpha_poly = ap_sdr$total$raw_alpha,
    omega_total = wt_sdr
  ), digits = 3, caption = "SDR-5 reliability (alpha, polychoric alpha, omega total).") 

  # Optional CFA: 1 factor
  sdr_model <- 'SDR =~ SDR1 + SDR2 + SDR3 + SDR4 + SDR5'
  sdr_fit <- try(lavaan::cfa(sdr_model, data = sdr_df, ordered = colnames(sdr_df), estimator = "WLSMV"), silent = TRUE)
  if (!inherits(sdr_fit, "try-error")) {
    print(fitMeasures(sdr_fit, c("cfi","tli","rmsea","srmr")))
  } else {
    cat("\nSDR-5 CFA skipped (model failed to converge).\n")
  }
}
```


# Step 9 — Cross-construct correlations (HPT, KN, FR-LF, KSA-3, SDR-5)

Why:
Useful overview to see how constructs relate before multilevel models.

```{r,cross-construct-correlations}
# Build scale scores that exist in your data (gracefully skipping any missing block)
scales_list <- list(
  HPT_total = hpt_scores$HPT_total,
  HPT_POP   = hpt_scores$POP,
  HPT_ROA   = hpt_scores$ROA,
  HPT_CONT  = hpt_scores$CONT
)

if (has_kn) {
  scales_list$KN_total <- rowSums(dat[keep, kn_cols], na.rm = TRUE)
}

if (has_fr) {
  fr_df <- dat[keep, fr_cols]
  scales_list$FR_RD     <- rowMeans(fr_df[, paste0("RD",1:3)], na.rm = TRUE)
  scales_list$FR_NS     <- rowMeans(fr_df[, paste0("NS",1:3)], na.rm = TRUE)
  scales_list$FR_total  <- rowMeans(fr_df, na.rm = TRUE)
}

if (has_ksa) {
  ksa_df <- dat[keep, ksa_cols]
  scales_list$KSA_A     <- rowMeans(ksa_df[, paste0("A",1:3)], na.rm = TRUE)
  scales_list$KSA_U     <- rowMeans(ksa_df[, paste0("U",1:3)], na.rm = TRUE)
  scales_list$KSA_K     <- rowMeans(ksa_df[, paste0("K",1:3)], na.rm = TRUE)
  scales_list$KSA_total <- rowMeans(ksa_df, na.rm = TRUE)
}

if (has_sdr) {
  sdr_df <- dat[keep, sdr_cols]
  scales_list$SDR_total <- rowMeans(sdr_df, na.rm = TRUE)
}

scales_df <- as_tibble(scales_list)

# Pairwise complete correlations
cors <- cor(scales_df, use = "pairwise.complete.obs")

print_tbl(round(cors, 3), caption = "Cross-construct correlations (pairwise complete).")
```

# Reproducibility appendix

```{r,session-info}
sessionInfo()
```